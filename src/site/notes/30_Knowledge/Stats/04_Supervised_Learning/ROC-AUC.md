---
{"dg-publish":true,"permalink":"/30-knowledge/stats/04-supervised-learning/roc-auc/","tags":["machine-learning","supervised"]}
---


## Definition

> [!abstract] Core Statement
> **ROC-AUC** (Receiver Operating Characteristic - Area Under Curve) measures the ==ability of a classifier to distinguish between classes== across all possible thresholds. AUC = 1.0 is perfect, AUC = 0.5 is random guessing.

![ROC Curve](https://upload.wikimedia.org/wikipedia/commons/3/36/ROC_space-2.png)

---

## Intuition (ELI5)

> [!tip] The Ranking Game
> AUC answers: "If I pick a random positive and a random negative example, what's the probability my model ranks the positive higher?" AUC = 0.8 means 80% of the time, positives score higher than negatives.

> [!tip] The "Sensitivity Knob" Metaphor
> Imagine a metal detector. You can turn the sensitivity up (find more gold, but beep at old nails) or down (no false beeps, but miss some gold). ROC-AUC tells you **how good the detector is overall**, regardless of where you set the knob.

---

## ROC Curve Components

| Axis | Metric | Formula |
|------|--------|---------|
| **X-axis** | False Positive Rate (FPR) | FP / (FP + TN) |
| **Y-axis** | True Positive Rate (TPR) | TP / (TP + FN) |

The curve is generated by varying the classification **threshold** from 0 to 1.

---

## Interpretation Guide

| AUC Value | Interpretation |
|-----------|----------------|
| **1.0** | Perfect classifier |
| **0.9-1.0** | Excellent |
| **0.8-0.9** | Good |
| **0.7-0.8** | Fair |
| **0.5-0.7** | Poor |
| **0.5** | Random guessing |
| **< 0.5** | Worse than random (flip predictions!) |

---

## Python Implementation

```python
from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt

# ========== TRAIN MODEL ==========
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
model = LogisticRegression()
model.fit(X_train, y_train)

# ========== PREDICT PROBABILITIES ==========
y_pred_proba = model.predict_proba(X_test)[:, 1]

# ========== CALCULATE AUC ==========
auc = roc_auc_score(y_test, y_pred_proba)
print(f"ROC-AUC: {auc:.4f}")

# ========== PLOT ROC CURVE ==========
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.5)')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# ========== SKLEARN DISPLAY ==========
RocCurveDisplay.from_estimator(model, X_test, y_test)
plt.show()

# ========== MULTI-CLASS AUC ==========
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_auc_score

# For multiclass, use 'ovr' (one-vs-rest)
# auc_multi = roc_auc_score(y_test, y_pred_proba_multi, 
#                           multi_class='ovr', average='macro')
```

---

## R Implementation

```r
library(pROC)

# ========== CALCULATE AUC ==========
roc_obj <- roc(y_test, y_pred_proba)
auc_value <- auc(roc_obj)
print(paste("AUC:", round(auc_value, 4)))

# ========== PLOT ==========
plot(roc_obj, main = "ROC Curve", 
     print.auc = TRUE, auc.polygon = TRUE)

# ========== CONFIDENCE INTERVAL ==========
ci.auc(roc_obj)

# ========== COMPARE MODELS ==========
# roc1 <- roc(y_test, pred_model1)
# roc2 <- roc(y_test, pred_model2)
# roc.test(roc1, roc2)  # DeLong test
```

---

## ROC-AUC vs Precision-Recall AUC

| Metric | Best For | Caveat |
|--------|----------|--------|
| **ROC-AUC** | Balanced classes | Overly optimistic for imbalanced |
| **PR-AUC** | Imbalanced classes | Focuses on positive class |

> [!warning] Imbalanced Data
> For highly imbalanced data (1% positives), ROC-AUC can be misleading.
> Use **Precision-Recall AUC** instead.

```python
from sklearn.metrics import precision_recall_curve, average_precision_score

precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
pr_auc = average_precision_score(y_test, y_pred_proba)
```

---

## Common Pitfalls

> [!warning] Real-World Traps
>
> **1. High AUC ≠ Good Predictions**
> - *Problem:* AUC is threshold-independent; actual predictions might be bad
> - *Solution:* Also check calibration, F1 at chosen threshold
>
> **2. Imbalanced Data Illusion**
> - *Problem:* 99% negatives → FPR stays low even with many false positives
> - *Solution:* Use PR-AUC or balanced accuracy
>
> **3. Comparing AUCs Across Datasets**
> - *Problem:* AUC depends on class distribution
> - *Solution:* Compare on same test set

---

## Related Concepts

- [[30_Knowledge/Stats/04_Supervised_Learning/Precision\|Precision]] — Numerator for PR curve
- [[30_Knowledge/Stats/04_Supervised_Learning/Recall\|Recall]] — Y-axis in both curves
- [[30_Knowledge/Stats/04_Supervised_Learning/F1 Score\|F1 Score]] — Threshold-dependent alternative
- [[30_Knowledge/Stats/04_Supervised_Learning/Imbalanced Data\|Imbalanced Data]] — When to use PR-AUC
- [[30_Knowledge/Stats/03_Regression_Analysis/Binary Logistic Regression\|Binary Logistic Regression]] — Common use case

---

## When to Use

> [!success] Use ROC-AUC When...
> - Refer to standard documentation
> - Refer to standard documentation

---

## When NOT to Use

> [!danger] Do NOT Use When...
> - Dataset is too small for training
> - Interpretability is more important than accuracy

---

## References

- **Paper:** Hanley, J. A., & McNeil, B. J. (1982). The meaning and use of the area under a receiver operating characteristic (ROC) curve. *Radiology*.
- **Tutorial:** [Scikit-learn ROC Curve](https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics)
